---
title: "Project 2 Jin"
---

## Project 2 Part 1

Practice using a API and making data visualizations.

The API we will use is `tidycensus` (<https://walker-data.com/tidycensus>), which is an R package that allows users to interface with a select number of the US Census Bureau’s data APIs and return tidyverse-ready data frames, optionally with simple feature geometry included.

The goal of this part is to create a set of data visualizations using the US Census Bureau’s data.

```{r}
############################################################
## Part 1 – tidycensus API (ACS data)
## Median household income over time by county in MD
############################################################

# install.packages(c("tidyverse", "tidycensus", "janitor", "sf"))  # <- run in Console if not installed

library(tidyverse)   # dplyr, ggplot2, readr, etc.
library(tidycensus)  # get_acs()
library(janitor)     # clean_names()
library(sf)          # st_drop_geometry()
library(purrr)       # map_dfr()

# Make a "data" folder (for saving .rds files) if it doesn't exist yet
if (!dir.exists("data")) dir.create("data")

## ---------------
## 1. MY QUESTION 
## ---------------
#
#   How has median household income changed over time across counties
#   in Maryland, and how does Baltimore City compare to other counties?
#
#   - Variable(s) median_income
#   - Geography counties in Maryland
#   - What years to compare: 2013, 2018, 2023

# Years to pull
years <- c(2013, 2018, 2023)

# State to analyze 
state_abbrev <- "MD"

# File name where we will save the combined income data
acs_file <- file.path("data", "md_county_income_2013_2018_2023.rds")

if (file.exists(acs_file)) {
  # If we've already downloaded & saved it, just load it
  md_income <- readRDS(acs_file)
} else {
  # If not, we call the Census API.
  # map_dfr() will:
  #  - loop over each year in 'years'
  #  - call get_acs() once per year (so 3 API calls total)
  #  - stack (row-bind) the results into one data frame

  md_income <- map_dfr(
    years,
    ~ get_acs(
      geography = "county",
      state     = state_abbrev,
      variables = c(median_income = "B19013_001"),  # ACS code for median household income
      year      = .x,
      survey    = "acs5",
      geometry  = TRUE   # include shapes for mapping later
    ) |>
      mutate(year = .x)  # add the year column
  )

  # Save locally so we don't repeat the API calls
  saveRDS(md_income, acs_file)
}

# Quick look at the structure (safe: not printing every row)
glimpse(md_income)

## ---------------------------------------------------------
## Extra dataset: total population (another set of API calls)
## ---------------------------------------------------------
# We also need a second dataset to join with (e.g., total population).
# This again uses map_dfr(), so we are using purrr in at least two places.

pop_file <- file.path("data", "md_county_population_2013_2018_2023.rds")

if (file.exists(pop_file)) {
  md_pop <- readRDS(pop_file)
} else {
  md_pop <- map_dfr(
    years,
    ~ get_acs(
      geography = "county",
      state     = state_abbrev,
      variables = c(total_pop = "B01003_001"),  # ACS code for total population
      year      = .x,
      survey    = "acs5",
      geometry  = FALSE   # numbers only; we don't need shapes here
    ) |>
      mutate(year = .x)
  )

  saveRDS(md_pop, pop_file)
}

glimpse(md_pop)

## ---------------------------------------------------------
## Join income + population into a single clean dataset
## ---------------------------------------------------------

md_acs <- md_income |>
  st_drop_geometry() |>  # drop sf geometry so we can do a regular join
  left_join(
    md_pop |>
      select(GEOID, year, total_pop = estimate),
    by = c("GEOID", "year")
  ) |>
  clean_names()          # make all column names snake_case

glimpse(md_acs)

## At this point:
## - md_income: income + geometry (good for maps)
## - md_pop:    population
## - md_acs:    joined table (income + population, no geometry)


```

```{r}
############################################################
## Part 1 – Wrangling + Plots (using md_income, md_acs)
############################################################

library(tidyverse)
library(sf)
library(janitor)

## 1. Prepare a simpler table for plotting (no geometry)

md_acs_small <- md_acs |>
  select(
    geoid     = geoid,
    name      = name,
    year      = year,
    median_income = estimate,
    median_income_moe = moe,
    total_pop
  ) |>
  # Make a cleaner county name (drop ", Maryland")
  mutate(
    county = str_remove(name, ", Maryland"),
    county = as.factor(county)
  )

# Check the structure
glimpse(md_acs_small)

# Latest year in your data (should be 2023)
latest_year <- max(md_acs_small$year, na.rm = TRUE)
latest_year

# Subset to the latest year for some plots
md_latest <- md_acs_small |>
  filter(year == latest_year) |>
  arrange(desc(median_income))

md_latest

############################################################
## PLOT 1 – Line plot over time by county
## geom_line + geom_point
############################################################

ggplot(md_acs_small, aes(x = year,
                         y = median_income,
                         group = county,
                         color = county)) +
  geom_line() +
  geom_point() +
  labs(
    title    = "Median Household Income Over Time by County",
    subtitle = paste("Maryland counties – ACS 5-year,", 
                     paste(range(md_acs_small$year), collapse = "–")),
    x        = "Year",
    y        = "Median household income (USD)",
    color    = "County",
    caption  = "Source: ACS 5-year via tidycensus"
  ) +
  theme_minimal()

############################################################
## PLOT 2 – Bar plot for the latest year
## geom_col
############################################################

ggplot(md_latest,
       aes(x = reorder(county, median_income),
           y = median_income)) +
  geom_col() +
  coord_flip() +
  labs(
    title    = paste("Median Household Income by County in", latest_year),
    subtitle = "Maryland counties (ACS 5-year)",
    x        = "County",
    y        = "Median household income (USD)",
    caption  = "Source: ACS 5-year via tidycensus"
  ) +
  theme_minimal()

############################################################
## Highlight Baltimore City vs others (dot plot)
############################################################

md_latest_baltimore <- md_latest |>
  mutate(
    is_baltimore_city = county == "Baltimore city"
  )

ggplot(md_latest_baltimore,
       aes(x = median_income,
           y = reorder(county, median_income),
           color = is_baltimore_city)) +
  geom_point(size = 3) +
  scale_color_manual(
    values = c("FALSE" = "grey50", "TRUE" = "red"),
    labels = c("Other counties", "Baltimore City"),
    name   = ""
  ) +
  labs(
    title    = paste("Median Income in", latest_year, 
                     "– Baltimore City vs Other Counties"),
    x        = "Median household income (USD)",
    y        = "County",
    caption  = "Source: ACS 5-year via tidycensus"
  ) +
  theme_minimal()

############################################################
## PLOT 3 – Choropleth map for the latest year
## geom_sf
############################################################

# Go back to the sf object with geometry
md_income_latest <- md_income |>
  filter(year == latest_year) |>
  clean_names()  # gives us 'estimate' and 'name' in snake_case

ggplot(md_income_latest) +
  geom_sf(aes(fill = estimate), color = "white") +
  labs(
    title    = paste("Median Household Income by County in", latest_year),
    subtitle = "Maryland counties – ACS 5-year",
    fill     = "Income (USD)",
    caption  = "Source: ACS 5-year via tidycensus"
  ) +
  theme_minimal()



```

**PART 1 REPORT of data analysis**

In this analysis, I used the ACS 5-year estimates from the tidycensus package to examine changes in median household income across all Maryland counties for the years 2013, 2018, and 2023.

I pulled county-level data for the median income variable (B19013_001) and combined the three years into one dataset using map_dfr(), then joined in the total population (B01003_001) for basic context about county size.

I then created a set of visualizations: a line plot showing how median income has changed over time for each county, a bar chart comparing counties in the most recent year, and a choropleth map to display the geographic pattern of income in 2023, with an additional plot highlighting Baltimore City relative to other counties.

**PART 1 REPORT of findings**

Overall, incomes tend to increase over time in most counties. Still, the plots show persistent gaps between higher-income and lower-income counties, with Baltimore City falling below many of its suburban neighbors in 2023.

These results should be interpreted cautiously because ACS estimates are subject to sampling error (as reflected in the margins of error), I used only three time points, and I did not adjust for inflation or differences in local cost of living, all of which can affect how meaningful the dollar differences between counties really are.

## Project 2 Part 2

Use the `rvest` package to scrape data from a website, wrangle and analyze the data, and summarize your findings.

```{r}
############################################################
## Part 2 – Web Scraping Project
## World Series champions from Wikipedia
############################################################

## Install packages once in the Console if needed:
# install.packages(c("rvest", "dplyr", "stringr", "ggplot2", "forcats"))

library(rvest)      # web scraping
library(dplyr)      # data wrangling
library(stringr)    # string cleaning
library(ggplot2)    # plotting
library(forcats)    # factor reordering

############################################################
## 1. Read HTML
############################################################

# Wikipedia page with a World Series champions table
url <- "https://en.wikipedia.org/wiki/List_of_World_Series_champions"

# Read the full HTML from the page
ws_page <- read_html(url)

############################################################
## 2. Extract table(s) with rvest
############################################################

# Extract all HTML tables into a list of data frames
all_tables <- ws_page |>
  html_table(fill = TRUE)

length(all_tables)  # just to see how many tables were found

# Automatically find the table that has Year, Winning team, Losing team
ws_raw_list <- Filter(
  function(df) all(c("Year", "Winning team", "Losing team") %in% names(df)),
  all_tables
)

# Take the first matching table
ws_raw <- ws_raw_list[[1]]

# Quick peek at the raw table structure
glimpse(ws_raw)

############################################################
## 3. Clean the data into a tidy format
############################################################

ws_clean <- ws_raw |>
  # keep just the columns we care about
  select(Year, `Winning team`, `Losing team`) |>
  # keep only rows where Year looks like a 4-digit year
  filter(str_detect(Year, "^\\d{4}$")) |>
  # convert Year to numeric and tidy team names
  mutate(
    Year           = as.numeric(Year),
    `Winning team` = str_squish(`Winning team`),
    `Losing team`  = str_squish(`Losing team`)
  )

glimpse(ws_clean)
# head(ws_clean)

############################################################
## 4. Analysis & Visualization: Top 5 teams
############################################################


# 1) Create a cleaned franchise name without the record in parentheses
ws_clean_franchise <- ws_clean |>
  mutate(
    team = `Winning team` |>
      str_squish() |>                # fix spacing
      str_remove("\\s*\\(.*$")       # remove space + everything in parentheses
  )

# 2) Count number of championships by franchise
win_counts <- ws_clean_franchise |>
  count(team, sort = TRUE)

win_counts |> head(10)  

# 3) Get the top 5 teams
top_5_teams <- win_counts |>
  slice_max(order_by = n, n = 5)

top_5_teams

# 4) Plot: horizontal bar chart for top 5
ggplot(top_5_teams,
       aes(x = n, y = fct_reorder(team, n))) +
  geom_col() +
  geom_text(aes(label = n), hjust = -0.2) +
  labs(
    title    = "Top 5 Teams by World Series Wins",
    subtitle = "Data scraped from Wikipedia (franchises grouped)",
    x        = "Number of World Series championships",
    y        = "Team"
  ) +
  xlim(0, max(top_5_teams$n) + 3) +
  theme_minimal()

```

**PART 2 REPORT of data analysis**

I scraped data from the Wikipedia page “List of World Series champions” using the rvest package in R.

The workflow first loaded the page HTML with read_html() and used html_table(fill = TRUE) to extract all tabular content. From the resulting list of tables, I programmatically selected the one that contained the columns Year, Winning team, and Losing team, which makes the code more robust to changes in table order.

I then cleaned the data with dplyr and stringr: keeping only rows where Year matched a four-digit year, converting Year to numeric, and removing the parenthetical performance summaries from team names (e.g., converting "New York Yankees (12, 9–3)" to "New York Yankees").

This produced a tidy dataset of championship years and winning/losing franchises, which I then summarized with count() and visualized using a horizontal bar chart (ggplot2) for the franchises with the most World Series titles.

**PART 2 REPORT of findings**

The analysis shows a highly skewed distribution of championships. The New York Yankees dominate with 27 World Series wins, far ahead of the St. Louis Cardinals (9 wins) and the Los Angeles Dodgers (8 wins), followed by the Boston Red Sox (7 wins) and a cluster of teams such as the Cincinnati Reds, Philadelphia Athletics, and Pittsburgh Pirates at 5 wins each. These results highlight how a small number of franchises account for a large share of championships.

However, there are several limitations and potential biases.

-   First, the scraper depends on the current HTML structure of the page—if column names or layout change, the selection logic could fail or pull the wrong table.

-   Second, the cleaning step that strips parentheses assumes a particular formatting convention; if the text format changes, franchise names might not be parsed correctly.

-   Finally, the analysis treats every cleaned name as a separate franchise and does not attempt to reconcile historical relocations or renamings (e.g., Brooklyn vs. Los Angeles Dodgers), so counts represent titles under the scraped name rather than fully aggregated franchise histories.
